# Detection Engineering: Automating Purple Team Operations at Triskele Lab

<aside>
ðŸ’¡

During my internship, I created a proof of concept to evolve our threat validation from manual testing to an automated, scalable Purple Team framework. I began by architecting a full-stack ELK SIEM environment on Linux and developing custom KQL detection rules to monitor attacks from a Windows origin. Identifying a significant operational bottleneck where manual testing took up to four hours per client I pivoted to MITRE Caldera, engineering custom plugins that transformed attack chains into one-click, autonomous operations. My final phase focused on future-proofing these assets by researching their integration into OpenAEV, ensuring our custom automation logic is ready for Triskeleâ€™s next-generation purple teaming platform.

</aside>

---

[https://github.com/L1quidDroid/Internship_C](https://github.com/L1quidDroid/Internship_C)

## Phase 1. Possessing a Detection Engineering Mindset

### Detection Engineering Resource Tracker

[Detection Engineering Task Tracker.csv](https://www.notion.so/3024834f7fc780b4beeae6ed07e79553?pvs=21)

My internship began by internalising core detection engineering philosophies to shift focus from finding malicious activity to maximising the operational cost for the attacker. By studying the Pyramid of Pain, I learnt to prioritised ingrained adversary behaviours and Tactics, Techniques, and Procedures over fragile indicators like file hashes or IP addresses. This strategic approach forces an adversary to reinvent their entire methodology to remain undetected. I applied the Funnel of Fidelity and Capability Abstraction to conceptualise how high volume telemetry is distilled into high confidence alerts. This mindset treats every false positive as a logic bug requiring refinement to prevent alert fatigue. By moving away from tool specific signatures toward understanding underlying behaviours like LSASS memory dumping, I ensured the resulting detection logic remained resilient against attack variations.

### Lab fundamentals

The initial phase of my internship focused on building a testing environment to serve as the foundation for all subsequent security research. Within the first two weeks, I deployed a fully functional detection laboratory from scratch, designed to mirror the core architecture of TLâ€™s Artemis SIEM. While my previous experience with home laboratories made the initial setup straightforward, the professional context required a much higher degree of precision. This stage marked my first significant deep dive into Kibana Query Language (KQL), where my focus shifted from collecting data to the art of detection engineering: filtering out system noise to create high-fidelity, actionable alerts for analysis.

### Simple Manual Purple Team Architecture

I established a controlled purple team environment using a dual-VM approach to bridge the gap between offensive execution and defensive analysis.

- Windows VM (Attack Origin): Utilised Atomic Red Team to simulate adversary techniques, with Sysmon and Winlogbeat capturing and shipping granular telemetry.
- Linux VM (Defensive Stack): Hosted the ELK stack to function as the central nervous system for storage and visualisation.

### View Initial Architecture

![Figure 1: Simple Initial Purple Team Config Architecture. Telemetry flow from Windows endpoint to Linux-based SIEM.](Detection%20Engineering%20Automating%20Purple%20Team%20Opera/CALDERA_Web_UI_Data_Flow-2026-02-10-074106.png)

Figure 1: Simple Initial Purple Team Config Architecture. Telemetry flow from Windows endpoint to Linux-based SIEM.

### Engineering Hurdles and Challenges

The deployment of the laboratory environment presented technical hurdles in resource management and log tuning. To maintain system stability, I optimised ELK JVM heap sizes to ensure a consistent 4GB of RAM headroom, which prevented data loss during heavy ingestion. I also addressed high volumes of telemetry noise generated by initial simulations by applying contextual filtering and refining Winlogbeat configurations. This achieved a significant reduction in environmental noise and was essential for isolating specific behavioural triggers.

Because physical VM access was restricted to office attendance, I recreated the same architecture in my personal cloud environment to maintain project momentum. Rather than repeating a manual installation, I utilised Bicep to define the Azure infrastructure as code. I encountered a significant roadblock regarding the resource limits of a student Azure account as the available quotas were insufficient to host a virtual machine with the processing power required to run the full ELK stack and Caldera simultaneously. Despite these hardware restrictions, the exercise was invaluable for establishing configuration parity between my professional and personal environments.

### View Infrastructure as Code (Bicep)

<aside>
ðŸ’¡

This repository represents the initial development phase of Caldera environment. As the project progressed, I identified significant scope creep and architectural complexities that threatened the efficiency of the project. To ensure a higher standard of code quality and alignment with the final project goals within the internship time frame, I made the decision to archive this version and roll back development. This allowed for a more focused, scalable second iteration that prioritised architectural clarity over feature bloat.

[](https://github.com/L1quidDroid/caldera/tree/master/bicep)

</aside>

Examples of Bicep IaC used to replicate initial purple team environment within my personal cloud enviroment. 

### **1. Security-Focused: NSG Telemetry Flow Control**

**File:** `modules/network.bicep` (Lines 130-145)

```bash
{
  name: 'AllowBeats'
  properties: {
    priority: 130
    direction: 'Inbound'
    access: 'Allow'
    protocol: 'Tcp'
    sourcePortRange: '*'
    destinationPortRange: '5044'
    sourceAddressPrefix: '10.0.0.0/16'
    destinationAddressPrefix: '*'
    description: 'Allow Filebeat from VNet'
  }
}
```

### **2. Efficiency-Focused: Environment-Driven VM Sizing**

**File:** `main.bicep` (Lines 71-86)

```bash
var vmSizes = {
  dev: {
    calderaElk: 'Standard_D2s_v3' // available in japaneast; 2 vCPU fits student quota with agents off
    agent: 'Standard_B1s'
  }
  stage: {
    calderaElk: 'Standard_D8s_v3'
    agent: 'Standard_D2s_v5'
  }
  'prod-lab': {
    calderaElk: 'Standard_E8s_v3'
    agent: 'Standard_D2s_v5'
  }
}
```

### **3. Resource-Focused: Isolated VNet with Role-Based Subnetting**

**File:** `modules/network.bicep` (Lines 19-57)

```bash
resource vnet 'Microsoft.Network/virtualNetworks@2023-05-01' = {
  name: vnetName
  location: location
  tags: tags
  properties: {
    addressSpace: {
      addressPrefixes: [
        '10.0.0.0/16'
      ]
    }
    subnets: [
      {
        name: calderaSubnetName
        properties: {
          addressPrefix: '10.0.1.0/24'
          networkSecurityGroup: {
            id: nsgCaldera.id
          }
          privateEndpointNetworkPolicies: 'Disabled'
        }
      }
      {
        name: elkSubnetName
        properties: {
          addressPrefix: '10.0.2.0/24'
          networkSecurityGroup: {
            id: nsgElk.id
          }
        }
      }
      {
        name: agentsSubnetName
        properties: {
          addressPrefix: '10.0.3.0/24'
          networkSecurityGroup: {
            id: nsgAgents.id
          }
        }
      }
    ]
  }
}
```

---

## Phase 2. **Building an Automated Purple Team Platform with Caldera**

### Executive Summary:

<aside>
ðŸ’¡

By transitioning from manual command strings to an integrated Caldera orchestration layer, I engineered a "one-click" autonomous feedback loop. This system functions on an event-driven engine to execute TTPs while simultaneously injecting ECS-compliant metadata (`purple.*` namespace) into the ELK Stack. The result is a closed-loop system that automates everything from command attack execution, granular tagging to the generation of PDF detection reports.

</aside>

### Bridging the Gap: Pivoting from Manual Scripts to Autonomous Orchestration

During the initial phase of my research, I utilised Atomic Red Team to conduct purple team activities, but I quickly identified that the manual execution of command strings was both arduous and inefficient. Validating a single attack technique required multiple manual steps, including checking prerequisites, executing the payload, and performing post-attack cleanup. This repetitive process hindered the speed of validation and introduced significant potential for human error during testing scenarios.

Shadowing a live purple team engagement further highlighted these operational bottlenecks. The existing workflow involved manually accessing client environments, installing tools, and synchronising data across multiple platforms such as Halo, Vectr, and PowerBI. Under this manual model, a detection engineer could only complete approximately five attack techniques within a four hour window. Recognising this as a major constraint on scalability, I implemented  Caldera to automate these command sequences. By engineering custom plugins to handle metadata insertion into the ELK SIEM and automating the closure of alerts, I successfully transformed a fragmented, multi-hour workflow into a streamlined, one click operation. This transition increases our testing capacity and allows for the immediate generated client reports with the necessary statistics directly within the platform.

### Technical Proof: Proof of Concept Demonstration

![Full User journey.  Show casing Autonomous validation from Caldera execution to ELK metadata ingestion into ELK](Detection%20Engineering%20Automating%20Purple%20Team%20Opera/ezgif.com-added-text(3).gif)

Full User journey.  Show casing Autonomous validation from Caldera execution to ELK metadata ingestion into ELK

### **System Design: Caldera Implementation Components &  Data Flows**

<aside>
ðŸ› 

Caldera facilitates a closed-loop validation cycle by bridging the gap between offensive execution and defensive observation. By transitioning from manual command strings to an integrated orchestration layer, the system enables agents to execute TTPs on the target Windows VM while simultaneously injecting metadata into the ELK Stack. This ensures that every simulated attack is uniquely tagged with its corresponding MITRE ATT&CK technique and operation ID, allowing for detection correlations.

</aside>

![Figure 2: By establishing a direct C2 channel to execute TTPs while simultaneously shipping enriched telemetry to the ELK Stack, the system enables real-time correlation between offensive actions and high-fidelity behavioral alerts.](Detection%20Engineering%20Automating%20Purple%20Team%20Opera/CALDERA_Web_UI_Data_Flow-2026-02-10-074027.png)

Figure 2: By establishing a direct C2 channel to execute TTPs while simultaneously shipping enriched telemetry to the ELK Stack, the system enables real-time correlation between offensive actions and high-fidelity behavioral alerts.

| **Port** | **Protocol** | **Service** | **Purpose** |
| --- | --- | --- | --- |
| **8888** | **HTTP** | Caldera Main UI | Primary web interface and agent beacons |
| **8443** | **HTTPS** | Caldera SSL | Encrypted web access |
| **7010** | TCP | Sandcat P2P | Agent-to-agent proxy communication |
| **9200** | HTTP | Elasticsearch | SIEM data ingestion and queries |

### **Implementation: Building the Automation Engine**

### **Custom Orchestrator: Automated SIEM Tagging**

![CALDERA Web UI Data Flow-2026-02-10-080258.png](Detection%20Engineering%20Automating%20Purple%20Team%20Opera/CALDERA_Web_UI_Data_Flow-2026-02-10-080258.png)

<aside>
ðŸ› 

One of the core features in this system is the Orchestrator automatic SIEM taggin**g** that happens when an operation is executed. Every attack technique is enriched with MITRE ATT&CK metadata and injected into Elasticsearch in real-time.

</aside>

- **Event-Driven Architecture**: The plugin utilises a pub/sub pattern by subscribing to `operation.state_changed` and `link.status_changed`, allowing it to react instantly to the attack lifecycle.
- **Circuit Breaker Pattern**: To prevent system degradation, the engine monitors for ELK connectivity issues; after five consecutive failures, it "opens" the circuit and halts active API calls to the SIEM.
- **Fallback Logging**: When the SIEM is unavailable, the Orchestrator gracefully degrades by switching to local JSON file logging, ensuring no validation data is lost.
- **Input Sanitisation**: To maintain data integrity and prevent injection attacks, the engine uses regex validation to verify that every technique ID matches the MITRE ATT&CK format (e.g., `T1234` or `T1234.001`).
- **ECS-Compliant Schema**: All telemetry is mapped to a dedicated `purple.*` namespace, ensuring that custom data follows the Elastic Common Schema and remains isolated from standard environmental logs.

```python
File: `plugins/orchestrator/hook.py`  
# plugins/orchestrator/hook.py (Lines 66-91)

# Subscribe to operation events via event_svc
event_svc = services.get('event_svc')

if event_svc:
    # Listen for operation state changes (created, started, completed)
    await event_svc.observe_event(
        callback=orchestrator_svc.on_operation_state_changed,
        exchange='operation',
        queue='state_changed'
    )
    
    # Listen for operation completion
    await event_svc.observe_event(
        callback=orchestrator_svc.on_operation_completed,
        exchange='operation',
        queue='completed'
    )
    
    # Listen for individual attack execution (granular tagging)
    await event_svc.observe_event(
        callback=orchestrator_svc.on_link_status_changed,
        exchange='link',
        queue='status_changed'
    )
    
    log.info('Orchestrator event handlers registered:')
    log.info('operation/state_changed')
    log.info('operation/completed')
    log.info('link/status_changed (granular attack tagging)')

```

### **Standardising Purple Team Telemetry with ECS Compliance**

<aside>
ðŸ› 

To avoid conflicts with native Elasticsearch fields, I implemented a custom namespace called "purple." that contains all enriched purple team metadata. 

</aside>

```json
# plugins/orchestrator/app/elk_tagger.py (Lines 136-205)

def _build_metadata(self, operation) -> Dict[str, Any]:
    """
    Build ECS-compatible metadata from Caldera operation object.
    
    Uses purple.* namespace for ATT&CK fields to enable SIEM filtering:
    - purple.technique: T1078, T1059.001, etc.
    - purple.tactic: TA0001, TA0007, etc.
    - purple.operation_id: Caldera operation UUID
    - purple.detection_status: pending/detected/evaded
    """
    # Extract techniques and tactics from operation chain
    techniques = []
    tactics = []
    ability_names = []
    
    if hasattr(operation, 'chain') and operation.chain:
        for link in operation.chain:
            if hasattr(link, 'ability'):
                ability = link.ability
                if hasattr(ability, 'technique_id') and ability.technique_id:
                    techniques.append(ability.technique_id)
                if hasattr(ability, 'tactic') and ability.tactic:
                    tactics.append(ability.tactic)
                if hasattr(ability, 'name') and ability.name:
                    ability_names.append(ability.name)
    
    # Deduplicate and limit
    techniques_list = list(set(techniques))
    tactics_list = list(set(tactics))
    
    # Build ECS-compatible document with purple.* namespace
    metadata = {
        # ECS timestamp
        '@timestamp': datetime.utcnow().isoformat() + 'Z',
        
        # Purple team namespace (for SIEM filtering)
        'purple': {
            'technique': techniques_list[0] if techniques_list else None,
            'techniques': techniques_list,
            'tactic': tactics_list[0] if tactics_list else None,
            'tactics': tactics_list,
            'operation_id': str(operation.id),
            'operation_name': getattr(operation, 'name', 'Unknown'),
            'agent_id': getattr(operation, 'group', 'unknown'),
            'detection_status': 'pending', 
             # Updated by detection correlation
             
            'ability_count': len(ability_names),
            'technique_count': len(techniques_list),
            'status': getattr(operation, 'state', 'unknown')
        },
        
        # Tags for Kibana filtering (purple_T1078, purple_TA0007)
        'tags': (
            ['purple_team', 'caldera', 'tl_labs', 'simulation'] +
            [f'purple_{t}' for t in techniques_list[:50]] +
            [f'purple_{tac}' for tac in tactics_list[:20]]
        )
    }
    
    return metadata

```

**Example JSON Output** 

```json
{
  "@timestamp": "2026-02-10T14:23:45Z",
  "purple": {
    "technique": "T1078",
    "techniques": ["T1078", "T1059.001", "T1003.001"],
    "tactic": "TA0001",
    "tactics": ["TA0001", "TA0002", "TA0006"],
    "operation_id": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
    "operation_name": "Client_Engagement_2026-02-10",
    "agent_id": "red_team_windows_01",
    "detection_status": "pending",
    "ability_count": 15,
    "technique_count": 8,
    "status": "running"
  },
  "tags": [
    "purple_team",
    "caldera",
    "tl_labs",
    "simulation",
    "purple_T1078",
    "purple_T1059.001",
    "purple_T1003.001",
    "purple_TA0001",
    "purple_TA0002"
  ]
}
```

### **Automated Detection Reporting: Debrief-ELK Integration**

<aside>
ðŸŽ¯

This plugin automates the final stage of the purple team lifecycle by querying Elasticsearch to validate which techniques were successfully detected versus those that evaded existing security controls.

</aside>

**Key Plugin Features**:

- **Operation Correlation**: Plugin utilises the `purple.operation_id` to instantly correlate every attack technique executed within a specific engagement, ensuring data integrity across the entire operation.
- **Automated Aggregation**: The plugin performs aggregations on the `purple.technique` field, providing a real-time view of the "Detected vs. Evaded" status for each TTP.
- **Automated Reporting**: Leveraging the **ReportLab** library, the plugin generates professional, color-coded PDF reports that visualises detection coverage gaps and operation results

```python
# plugins/debrief-elk-detections/app/elk_fetcher.py
query = {
    'bool': {
        'must': [
            # Correlate logs to the specific simulation ID
            {'terms': {'purple.operation_id.keyword': ['a1b2c3d4-...']}},
            # Ensure we are only pulling enriched purple team techniques
            {'exists': {'field': 'purple.technique'}}
        ]
    }
}

# Executing the search with technique-based aggregations
response = await es.search(
    index='purple-team-logs-*',
    query=query,
    aggs={
        'techniques': {
            'terms': {'field': 'purple.technique.keyword'}
        }
    }
)
```

### Engineering Hurdles and Challenges

### Challenge 1: WebSocket Handling

<aside>
ðŸŽ¯

During the initial development of the Orchestrator plugin, the traditional approach of polling the `data_svc` every five seconds was a significant bottleneck. This method created unnecessary CPU overhead and delayed reactions, resulting in increased compute costs for the hosted Linux VM and operation lag. To overcome this, I pivoted to an event-driven architecture by subscribing directly to `operation.state_changed` via WebSockets. Refactoring the plugin to utilise asynchronous callbacks, resulting in  a real-time, decoupled, and scalable system that synchronises telemetry instantly without wasting cloud compute resources.

</aside>

```python
File: plugins/orchestrator/app/orchestrator_svc.py  

# plugins/orchestrator/app/orchestrator_svc.py (Lines 44-88)

async def on_operation_state_changed(self, socket, path, services):
    """
    Event handler: Tag operation when state changes.
    
    Called by Caldera event_svc when operation state changes.
    Subscribed to exchange='operation', queue='state_changed'.
    
    Args:
        socket: Websocket connection object (NOT a standard HTTP request!)
        path: Websocket path (e.g., '/operation/state_changed')
        services: Caldera service registry
    """
    try:
        # Read and parse JSON message from websocket
        message_data = await socket.recv()  # WebSocket-specific method
        event_data = json.loads(message_data)
        
        # Extract event parameters
        op_id = event_data.get('op')
        from_state = event_data.get('from_state')
        to_state = event_data.get('to_state')
        
        if not op_id:
            self.log.warning('[orchestrator] State change event missing operation ID')
            return
        
        self.log.info(f'[orchestrator] State change: {op_id[:16]}... ({from_state} â†’ {to_state})')
        
        # Fetch operation object from data_svc using ID
        data_svc = services.get('data_svc')
        if not data_svc:
            self.log.error('[orchestrator] data_svc not available')
            return
        
        operations = await data_svc.locate('operations', match=dict(id=op_id))
        if not operations:
            self.log.warning(f'[orchestrator] Operation not found: {op_id[:16]}...')
            return
        
        operation = operations[0]
        await self.elk_tagger.tag(operation)
        
    except Exception as e:
        # Non-fatal error (don't break operation)
        self.log.error(f'[orchestrator] Operation tagging failed (non-fatal): {e}', exc_info=True)
```
```

### Challenge 2: Circuit Breaker & Elasticsearch Unavailability

<aside>
ðŸŽ¯

During development, a recurring operational friction arose when I would launch a Caldera operation without ensuring the ELK stack was active. Because Caldera initially lacked a feedback mechanism for failed external connections, I spent considerable time debugging non-functional features before realising the SIEM was simply offline. To resolve this, I implemented a Circuit Breaker pattern that tracks connection health. By monitoring ingestion attempts, the system now identifies connectivity gaps after five failures, "opening" the circuit and providing a clear indication of the failure. This ensures graceful degradation by switching to local JSON fallback logging, preventing server instability and eliminating the manual debugging time previously wasted on silent connection errors.

</aside>

```python
File: plugins/orchestrator/app/elk_tagger.py  
# plugins/orchestrator/app/elk_tagger.py (Circuit breaker implementation)

# Initialise circuit breaker in __init__
self._failure_count = 0
self._max_failures = 5
self._circuit_open = False

# Tag operation with circuit breaker protection
async def tag(self, operation):
    """Tag operation in Elasticsearch with circuit breaker protection."""
    
    # Check if circuit is open (too many failures)
    if self._circuit_open:
        self.log.warning('Circuit breaker is open, using fallback logging')
        await self._log_to_fallback(metadata)
        return
    
    try:
        # Attempt to tag in Elasticsearch
        metadata = self._build_metadata(operation)
        metadata = self._sanitize_metadata(metadata)
        
        await self.elk_client.index(
            index=self.config.ELK_INDEX,
            document=metadata
        )
        
        # Reset failure count on success
        self._failure_count = 0
        self.log.debug(f'Tagged operation: {operation.id[:16]}...')
        
    except (ConnectionError, TransportError) as e:
        # Increment failure count
        self._failure_count += 1
        
        # Open circuit if too many failures
        if self._failure_count >= self._max_failures:
            self._circuit_open = True
            self.log.error(f'Circuit breaker opened after {self._max_failures} failures')
        
        # Fallback to file logging
        self.log.warning(f'ELK tagging failed ({self._failure_count}/{self._max_failures}): {e}')
        await self._log_to_fallback(metadata)
```

```

### Challenge 3: Input Injection Attacks

<aside>
ðŸŽ¯

To replicate a production ready system, treating all user-controlled data as untrusted is a fundamental security requirement. During development, I identified a significant risk where operation names or technique IDs could be manipulated to execute SQL injection attacks against the Elasticsearch backend. To maintain data integrity, I engineered a strict sanitisation layer using the `_sanitize_metadata` function. By implementing regex validation, the engine verifies that every technique ID strictly adheres to the MITRE ATT&CK format such as: `T1234` or `T1234.001` , while simultaneously stripping special characters and limiting field lengths to prevent potential Denial of Service (DoS) attempts. This ensures the telemetry stream remains secure and reliable, even when processing arbitrary input from the Caldera web interface.

</aside>

```python
File: plugins/orchestrator/app/elk_tagger.py  
# plugins/orchestrator/app/elk_tagger.py (Lines 100-134)

def _sanitize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    Sanitize metadata to prevent injection attacks.
    
    Security considerations:
    - Operation names could contain SQL/NoSQL injection attempts
    - Technique IDs must match MITRE ATT&CK format (T1234 or T1234.001)
    - Limit field lengths to prevent DOS attacks
    
    Args:
        metadata: Raw metadata dictionary
        
    Returns:
        Sanitized metadata
    """
    # Sanitize operation name (remove special chars, limit length)
    if 'operation_name' in metadata:
        metadata['operation_name'] = re.sub(r'[^\w\s-]', '', metadata['operation_name'])[:200]
    
    # Validate technique IDs (MITRE ATT&CK format: T1234 or T1234.001)
    if 'techniques' in metadata:
        metadata['techniques'] = [
            tid for tid in metadata['techniques']
            if re.match(r'^T\d{4}(\.\d{3})?$', str(tid))
        ]
    
    # Validate tactics (alphanumeric only)
    if 'tactics' in metadata:
        metadata['tactics'] = [
            tactic for tactic in metadata['tactics']
            if re.match(r'^[a-zA-Z0-9\s]+$', str(tactic))
        ]
    
    # Sanitize client_id (alphanumeric and underscore only)
    if 'client_id' in metadata:
        metadata['client_id'] = re.sub(r'[^\w-]', '', str(metadata['client_id']))[:100]
    
    return metadata
```

### Challenge 4:  **Eliminating Manual Commands Through Bash Scripts**

<aside>
ðŸ’¡

A recurring friction point during development was the manually managing the platform's lifecycle. Each session required a specific sequence of booting the Linux VM, waiting for the ELK stack to reach a healthy state, and manually launching the Caldera server while monitoring for port conflicts. To eliminate this manual process and reduce debugging time, I created multiple ****scripts to automate the environment's management. These scripts ensure configuration parity across sessions and provide an immediate health check of the entire stack.

[Internship_C/scripts at main Â· L1quidDroid/Internship_C](https://github.com/L1quidDroid/Internship_C/tree/main/scripts)

</aside>

- verify_sanity.sh: Conducts a pre-flight check of all Python imports, custom plugin files, and dependencies to prevent runtime errors.
- [tl-status.sh](http://tl-status.sh/): Provides a single point of view for the ELK stack healthchecks and Caldera service status.
- [tl-startup.sh](http://tl-startup.sh/): Optimises VM resources, triggers the ELK bootstrap, and launches Caldera once dependencies are live.
- [tl-shutdown.sh](http://tl-shutdown.sh/): Ensures a clean exit by stopping services in the correct order, clearing active ports, and archiving temporary logs.

---

## Phase 3. Future-Proofing: Automation with OpenAEV.

<aside>
ðŸ’¡

In the final phase of my internship, I researched the transition to OpenAEV to align with TLâ€™s move toward Purple Teaming. OpenAEV serves as the Strategic Brain, pulling real-time threat profiles from OpenCTI. Rather than replacing my existing work, OpenAEV utilises Caldera as a technical "Injector." This means my custom plugins such as Orchestrator and ****Debrief-ELK ****can be used as  Muscle, providing the granular SIEM telemetry and automated reporting that OpenAEV's strategic layer requires.

</aside>

### Thought Experiment on combining Caldera with OpenAEV

<aside>
ðŸ› 

OpenAEV functions as the strategic "Brain" of the framework, providing a high-level orchestration layer that ingests real-time threat intelligence from OpenCTI to trigger automated validation campaigns. Its modular architecture is built on a series of "Injectors" where Caldera serves as the engine for technique execution, effectively functioning as the "Muscle" of the system.

OpenAEV initiates simulations via the standard Caldera REST API, therefore, my plugins, specifically the Orchestrator and Debrief-ELK plugins, remain fully portable and functional to OpenAEV. These plugins operate at the server level. This means they will continue to provide the automated SIEM tagging, circuit-breaker resilience, and granular PDF reporting required for technical remediation regardless of whether the initial trigger originated from the Caldera UI or an autonomous OpenAEV scenario. By maintaining this stack, we ensure that as Triskele Labs scales to more sophisticated orchestration, our underlying technical validation remains secure, automated, and integrated into our SIEM telemetry.

</aside>

![Untitled diagram-2026-02-10-102556.png](Detection%20Engineering%20Automating%20Purple%20Team%20Opera/Untitled_diagram-2026-02-10-102556.png)

### Engineering Hurdles (Theoretical)

### Changes to Custom plugin API

To enable communication between OpenAEV's strategic layer and Caldera's tactical execution layer, the primary integration requirement is ID standardisation. Specifically, custom Ability IDs in Caldera must map directly to corresponding "Injects" defined in OpenAEV. OpenAEV triggers simulations by calling these unique identifiers via REST API, so maintaining configuration parity ensures the Orchestrator plugin correctly tags telemetry. This standardised ID mapping allows the plugin to identify incoming requests and continue providing real-time SIEM enrichment and automated reporting without requiring a fundamental rewrite of the underlying Python logic.

### **Data Normalisation**

Data normalisation is the second critical integration requirement, ensuring that OpenAEV's metrics and the SIEM's telemetry speak a common language. OpenAEV operates at the business layer with high-level "Expectation" results (Success/Failure), while Caldera's purple* namespace provides the granular technical detail needed for engineering remediation. By standardising field names and data types across the Caldera Orchestrator and SIEM, every purple simulation regardless of which orchestrator triggers it populates the SIEM with consistent, searchable metadata. This alignment is essential to ensure that regardless of which platform triggers the attack, the resulting data remains compatible with our automated reporting and detection logic.

## Closing remarks

My internship at Triskele Labs has been a transformative experience in understanding the Detection Engineering mindset and the effort required to defend modern environments. While my work with Caldera served as a learning proof of concept rather than a production-ready solution, the process of building it allowed me to navigate the full lifecycle of a security product a journey that was previously unknown to me. By engineering custom orchestration plugins and resolving complex hurdles like asynchronous WebSocket handling and Circuit Breaker patterns, I gained deep, hands-on insight into how purple teaming functions at scale and where the critical bottlenecks in manual testing truly lie.

I am incredibly grateful to the Detection Engineering team at Triskele Labs for the mentorship and the technical freedom to "deep dive" into this project. This opportunity allowed me to internalise how high-fidelity rules are written and, more importantly, how they are tuned through the "Funnel of Fidelity" to filter environmental noise and reduce analyst alert fatigue. I leave this internship with a secure foundation in security automation and a clear understanding of the art of detection engineering. I am proud of the technical challenges I overcame and am eager to apply these lessons and this proactive engineering mindset to the next stage of my career.